{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'convert_eprime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3db3655b4700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconvert_eprime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'convert_eprime'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from convert_eprime import convert as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.path.join('..','sourcedata')\n",
    "derivs_dir = os.path.join('..','derivatives','0.3.cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to clean the N-back data\n",
    "\n",
    "Stack the blocks vertically instead of horizontally, label the trial rows properly, and tag each trial as a HIT, MISS, FA, CR. We are also ouputting a new CSV data file in the sourcedata folder, all cleaned-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nstack_score_label(df):\n",
    "    \n",
    "    # Hierarchicalize the column index\n",
    "    df.columns=pd.MultiIndex.from_tuples([\n",
    "        (df.columns[0].split('.')[0],df.columns[0].split('.')[1]),\n",
    "        (df.columns[1].split('.')[0],df.columns[1].split('.')[1]),\n",
    "        (df.columns[2].split('.')[0],df.columns[2].split('.')[1]),\n",
    "        (df.columns[3].split('.')[0],df.columns[3].split('.')[1]),\n",
    "        (df.columns[4].split('.')[0],df.columns[4].split('.')[1]),\n",
    "        (df.columns[5].split('.')[0],df.columns[5].split('.')[1]),\n",
    "    ])\n",
    "    \n",
    "    # Stack blocks, Reset trial row index, and Rename columns to be descriptive\n",
    "    df = df.stack(0).reset_index().rename(\n",
    "        columns={'level_0':'trial','level_1':'block'}\n",
    "    ).sort_values(['block','trial'])\n",
    "    df['sub'] = os.path.basename(fpath).split('_')[0].split('-')[1]\n",
    "    df['block'] = df['block'].str[1]\n",
    "    df['trial'] = df['trial'] + 1\n",
    "    df = df.set_index(['sub','block','trial'])\n",
    "    \n",
    "    # Determine Hits, CRs, FAs\n",
    "    cr_mask = (df['Rsp'] == 0) & (df['CRsp'] == 0)\n",
    "    ms_mask = (df['Rsp'] == 0) & (df['CRsp'] == 1)\n",
    "    fa_mask = (df['Rsp'] == 1) & (df['CRsp'] == 0)\n",
    "    ht_mask = (df['Rsp'] == 1) & (df['CRsp'] == 1)\n",
    "    df['CR']   = cr_mask.astype(int)\n",
    "    df['MISS'] = ms_mask.astype(int)\n",
    "    df['FA']   = fa_mask.astype(int)\n",
    "    df['HIT']  = ht_mask.astype(int)\n",
    "    \n",
    "    # Convert RT 0 to RT NaN\n",
    "    df['RT'] = df['RT'].replace(0,np.NaN)\n",
    "    \n",
    "    # Output to new CSV datafile\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the subject data\n",
    "\n",
    "Reading only data for the full sample (100-series YA & 200-series OA). Executing N-back data cleaning & EPrime text-to-csv conversion. Setting up for subject-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep_frames=[]\n",
    "ptb_frames=[]\n",
    "\n",
    "for s in os.listdir(source_dir):\n",
    "    if s.startswith('sub-1') or s.startswith('sub-2'):\n",
    "        sub_dir = os.path.join(source_dir,s)\n",
    "        for f in os.listdir(sub_dir):\n",
    "            o = f.split('.')[0]+'.csv'\n",
    "            fpath = os.path.join(sub_dir,f)\n",
    "            outpath = os.path.join(sub_dir,o)\n",
    "            if f.split('_')[-1] == 'beh.txt':\n",
    "                print(f)\n",
    "                if os.path.isfile(outpath):\n",
    "                    print(os.path.basename(outpath),'exists')\n",
    "                else:\n",
    "                    ep.text_to_csv(fpath,outpath)\n",
    "                ep_frames.append(pd.read_csv(outpath))\n",
    "            if f.split('_')[-1] == 'beh.xlsx':\n",
    "                print(f)\n",
    "                if os.path.isfile(outpath):\n",
    "                    print(os.path.basename(outpath),'exists')\n",
    "                else:\n",
    "                    nstack_score_label(pd.read_excel(fpath)).to_csv(outpath)\n",
    "                    print('Output file successfully created- ',outpath)\n",
    "                ptb_frames.append(pd.read_csv(outpath))\n",
    "                ptb_frames[-1]['sub'] = f.split('_')[0].split('-')[1]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output N-back trial-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nback_trials = pd.concat(ptb_frames)\n",
    "fpath = os.path.join(derivs_dir,'nback_trial_level.csv')\n",
    "nback_trials.to_csv(fpath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group, expand, trim N-back data\n",
    "Group by subjects, get the sum of all columns, the count of the trial column, and the mean of the RT column.\n",
    "\n",
    "Establish Hit % `number of Hits / number of targets` and FA % `number of FAs / number of foils`. \n",
    "\n",
    "Corrected Recognition `HIT% - FA%`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nback_hits = nback_trials[nback_trials['HIT'] == 1]\n",
    "grouped_trials = nback_trials.groupby('sub')\n",
    "grouped_hits = nback_hits.groupby('sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nback_subs = grouped_trials.sum()\n",
    "nback_subs['trial'] = grouped_trials.count()['trial']\n",
    "nback_subs['RT'] = grouped_hits.mean()['RT']\n",
    "nback_subs['HIT%'] = nback_subs['HIT'] / nback_subs['CRsp']\n",
    "nback_subs['FA%'] = nback_subs['FA'] / (nback_subs['trial'] - nback_subs['CRsp'])\n",
    "nback_subs['CoR'] = nback_subs['HIT%'] - nback_subs['FA%']\n",
    "nback_subs = nback_subs[['RT','HIT%','FA%','CoR']]\n",
    "nback_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output N-back subject-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(derivs_dir,'nback_subject_level.csv')\n",
    "nback_subs.to_csv(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcSpd data\n",
    "Cleanup: Rehomogenize subject column, concatenate all frames, drop unneeded columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in ep_frames:\n",
    "    df['Subject'] = df['Subject'].iloc[-1]\n",
    "procspd_trials = pd.concat(ep_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procspd_trials = procspd_trials[procspd_trials['Procedure']=='TrialProc']\n",
    "procspd_trials = procspd_trials[[\n",
    "    'Subject','TargetStimulus.RT','TargetStimulus.OnsetToOnsetTime','Buffer.RT'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine response windows for final RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rt(row):\n",
    "    initial = row['TargetStimulus.RT']\n",
    "    buffert = row['Buffer.RT']\n",
    "    initial_duration = row['TargetStimulus.OnsetToOnsetTime']\n",
    "    if initial == 0 and buffert > 0:\n",
    "        rt = buffert + initial_duration\n",
    "    elif initial > 0: rt = initial\n",
    "    else: rt = np.nan\n",
    "    return(rt)\n",
    "\n",
    "procspd_trials['RT'] = procspd_trials.apply(calculate_rt,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output Procspd trial-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(derivs_dir,'procspd_trial_level.csv')\n",
    "procspd_trials.to_csv(fpath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group and output ProcSpd subject-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = procspd_trials.groupby('Subject')\n",
    "procspd_subs = grouped.mean()[['RT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(derivs_dir,'procspd_subject_level.csv')\n",
    "procspd_subs.to_csv(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "## Join all subject-level data\n",
    "Now that the subject-level data is cleaned & computed for ...\n",
    "- Survey measures\n",
    "- Comprehension\n",
    "- N-back\n",
    "- Processing speed\n",
    "\n",
    "... we can combine all of that data into our final subject-level data set: [0.4.join_subject_level.ipynb](0.4.join_subject_level.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
