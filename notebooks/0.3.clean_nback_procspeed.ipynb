{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from convert_eprime import convert as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = Path('..') / '..' / 'sourcedata'\n",
    "derivs_dir = Path('..') / '..' / 'derivatives' / '0.3.cleaned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to clean the N-back data\n",
    "\n",
    "Stack the blocks vertically instead of horizontally, label the trial rows properly, and tag each trial as a HIT, MISS, FA, CR. We are also ouputting a new CSV data file in the sourcedata folder, all cleaned-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nstack_score_label(infile):\n",
    "    \n",
    "    df = pd.read_excel(infile)\n",
    "    \n",
    "    # Hierarchicalize the column index\n",
    "    df.columns=pd.MultiIndex.from_tuples([\n",
    "        (df.columns[0].split('.')[0],df.columns[0].split('.')[1]),\n",
    "        (df.columns[1].split('.')[0],df.columns[1].split('.')[1]),\n",
    "        (df.columns[2].split('.')[0],df.columns[2].split('.')[1]),\n",
    "        (df.columns[3].split('.')[0],df.columns[3].split('.')[1]),\n",
    "        (df.columns[4].split('.')[0],df.columns[4].split('.')[1]),\n",
    "        (df.columns[5].split('.')[0],df.columns[5].split('.')[1]),\n",
    "    ])\n",
    "    \n",
    "    # Stack blocks, Reset trial row index, and Rename columns to be descriptive\n",
    "    df = df.stack(0).reset_index().rename(\n",
    "        columns={'level_0':'trial','level_1':'block'}\n",
    "    ).sort_values(['block','trial'])\n",
    "    df['sub'] = infile.name.split('_')[0].split('-')[1]\n",
    "    df['block'] = df['block'].str[1]\n",
    "    df['trial'] = df['trial'] + 1\n",
    "    df = df.set_index([\n",
    "        'sub',\n",
    "        'block',\n",
    "        'trial'\n",
    "    ])\n",
    "    \n",
    "    # Determine Hits, CRs, FAs\n",
    "    cr_mask = (df['Rsp'] == 0) & (df['CRsp'] == 0)\n",
    "    ms_mask = (df['Rsp'] == 0) & (df['CRsp'] == 1)\n",
    "    fa_mask = (df['Rsp'] == 1) & (df['CRsp'] == 0)\n",
    "    ht_mask = (df['Rsp'] == 1) & (df['CRsp'] == 1)\n",
    "    df['CR']   = cr_mask.astype(int)\n",
    "    df['MISS'] = ms_mask.astype(int)\n",
    "    df['FA']   = fa_mask.astype(int)\n",
    "    df['HIT']  = ht_mask.astype(int)\n",
    "    \n",
    "    # Convert RT 0 to RT NaN\n",
    "    df['RT'] = df['RT'].replace(0,np.NaN)\n",
    "    \n",
    "    # Output to new CSV datafile\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the subject data\n",
    "\n",
    "Reading only data for the full sample (100-series YA & 200-series OA). Executing N-back data cleaning & EPrime text-to-csv conversion. Setting up for subject-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_frames=[]\n",
    "ptb_frames=[]\n",
    "\n",
    "for sub_dir in source_dir.glob('sub-[1-2]*'):\n",
    "    for infile in sub_dir.glob('*'):\n",
    "        outfile = infile.parent / (infile.stem + '.csv')\n",
    "        if infile.name.split('_')[-1] == 'beh.txt':\n",
    "            print(infile)\n",
    "            if outfile.is_file():\n",
    "                print(outfile.name, 'exists')\n",
    "            else:\n",
    "                ep.text_to_csv(infile, outfile)\n",
    "            ep_frames.append(pd.read_csv(outfile))\n",
    "        if infile.name.split('_')[-1] == 'beh.xlsx':\n",
    "            print(infile)\n",
    "            if outfile.is_file():\n",
    "                print(outfile.name, 'exists')\n",
    "            else:\n",
    "                nstack_score_label(infile).to_csv(outfile)\n",
    "                print('Output file successfully created-', outfile)\n",
    "            ptb_frames.append(pd.read_csv(outfile))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output N-back trial-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nback_trials = pd.concat(ptb_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = derivs_dir / 'nback_trial_level.csv'\n",
    "nback_trials.to_csv(fpath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group, expand, trim N-back data\n",
    "Group by subjects, get the sum of all columns, the count of the trial column, and the mean of the RT column.\n",
    "\n",
    "Establish Hit % `number of Hits / number of targets` and FA % `number of FAs / number of foils`. \n",
    "\n",
    "Corrected Recognition `HIT% - FA%`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nback_hits = nback_trials[nback_trials['HIT'] == 1]\n",
    "grouped_trials = nback_trials.groupby('sub')\n",
    "grouped_hits = nback_hits.groupby('sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nback_subs = grouped_trials.sum()\n",
    "nback_subs['trial'] = grouped_trials.count()['trial']\n",
    "nback_subs['RT'] = grouped_hits.mean()['RT']\n",
    "nback_subs['HIT%'] = nback_subs['HIT'] / nback_subs['CRsp']\n",
    "nback_subs['FA%'] = nback_subs['FA'] / (nback_subs['trial'] - nback_subs['CRsp'])\n",
    "nback_subs['CoR'] = nback_subs['HIT%'] - nback_subs['FA%']\n",
    "nback_subs = nback_subs[['RT','HIT%','FA%','CoR']]\n",
    "nback_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output N-back subject-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = derivs_dir / 'nback_subject_level.csv'\n",
    "nback_subs.to_csv(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcSpd data\n",
    "Cleanup: Rehomogenize subject column, concatenate all frames, drop unneeded columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in ep_frames:\n",
    "    df['Subject'] = df['Subject'].iloc[-1]\n",
    "procspd_trials = pd.concat(ep_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procspd_trials = procspd_trials[procspd_trials['Procedure']=='TrialProc']\n",
    "procspd_trials = procspd_trials[[\n",
    "    'Subject','TargetStimulus.RT','TargetStimulus.OnsetToOnsetTime','Buffer.RT'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine response windows for final RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rt(row):\n",
    "    initial = row['TargetStimulus.RT']\n",
    "    buffert = row['Buffer.RT']\n",
    "    initial_duration = row['TargetStimulus.OnsetToOnsetTime']\n",
    "    if initial == 0 and buffert > 0:\n",
    "        rt = buffert + initial_duration\n",
    "    elif initial > 0: rt = initial\n",
    "    else: rt = np.nan\n",
    "    return(rt)\n",
    "\n",
    "procspd_trials['RT'] = procspd_trials.apply(calculate_rt,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output Procspd trial-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = derivs_dir / 'procspd_trial_level.csv'\n",
    "procspd_trials.to_csv(fpath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group and output ProcSpd subject-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = procspd_trials.groupby('Subject')\n",
    "procspd_subs = grouped.mean()[['RT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = derivs_dir / 'procspd_subject_level.csv'\n",
    "procspd_subs.to_csv(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patching in corrected exclusion of outlier trials\n",
    "\n",
    "Hard patch to correctly exclude individual outlier trials at 0.00135/0.99865 quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFU1Ua7frXah"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn as skl\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XxJ8KeTrpfe"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivs_dir = Path().absolute().parents[1] / 'derivatives'\n",
    "nb_trials_fpath = derivs_dir / '0.3.cleaned' / 'nback_trial_level.csv'\n",
    "ps_trials_fpath = derivs_dir / '0.3.cleaned' / 'procspd_trial_level.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9Y-uLSZrtSH"
   },
   "outputs": [],
   "source": [
    "nb_trials = pd.read_csv(nb_trials_fpath)\n",
    "ps_trials = pd.read_csv(ps_trials_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqte1jnYBRbZ"
   },
   "source": [
    "Best method so far below: \n",
    "\n",
    "- quantile method should be appropriate but I cannot find the right value for 3 SD\n",
    "- use hits only when calculating bounds, or use all trials? `base1` vs `base2`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "group_col = 'sub'\n",
    "value_col = 'RT'\n",
    "upper_Q = 0.95\n",
    "lower_Q = 0.05\n",
    "df = nb_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMI2Tz7x5F91"
   },
   "outputs": [],
   "source": [
    "from outliers import group_exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-back bound RT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_trials['RTbound'] = group_exclude(nb_trials, 'sub', 'RT')\n",
    "nb_hits = nb_trials.loc[nb_trials['HIT'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1174
    },
    "colab_type": "code",
    "id": "X46fzrfm1otK",
    "outputId": "6a601f34-a52a-4f4c-8726-8f38d8d16c72"
   },
   "outputs": [],
   "source": [
    "nb_rt = (nb_hits.groupby('sub')\n",
    "                .mean()['RTbound']\n",
    "                .rename('nback_RT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sub_fpath = derivs_dir / '0.3.cleaned' / 'nback_subject_level.csv'\n",
    "(pd.read_csv(nb_sub_fpath)\n",
    "     .merge(nb_rt.reset_index())).to_csv(nb_sub_fpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcSpeed bound RT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_trials['RTbound'] = group_exclude(ps_trials, 'Subject', 'RT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_RT = (ps_trials.groupby('Subject')\n",
    "                  .mean()['RTbound']\n",
    "                  .rename('procspd_RT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_sub_fpath = derivs_dir / '0.3.cleaned' / 'procspd_subject_level.csv'\n",
    "(pd.read_csv(ps_sub_fpath)\n",
    "     .merge(ps_RT.reset_index())).to_csv(ps_sub_fpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "## Join all subject-level data\n",
    "Now that the subject-level data is cleaned & computed for ...\n",
    "- Survey measures\n",
    "- Comprehension\n",
    "- N-back\n",
    "- Processing speed\n",
    "\n",
    "... we can combine all of that data into our final subject-level data set: [0.4.join_subject_level.ipynb](0.4.join_subject_level.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
